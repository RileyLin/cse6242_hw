# HW3 Q2 Implementation Guide
## From Local Development to Production Databricks

---

## ðŸ“‹ **Step-by-Step: What You Need to Do**

### **Phase 1: Local Testing (Optional but Recommended)**

1. **Test the code locally**:
   ```bash
   cd /Users/mxy1998/Documents/rileygit/6242_hw1/hw3/Q2
   # Make sure Docker container is running for Spark
   # Or install PySpark locally: pip install pyspark
   ```

2. **Run Python script to verify logic**:
   - Open `q2_solution.py` in your IDE
   - Each section is marked with CELL comments
   - Verify outputs match expected format

### **Phase 2: Transfer to Databricks**

1. **Access Databricks Community Edition**:
   - Go to: https://community.cloud.databricks.com
   - Log in with your account

2. **Import the template notebook**:
   - Click **Workspace** â†’ **Users** â†’ Your email
   - Click **Import**
   - Upload `q2.dbc` from hw3-skeleton folder
   - The notebook opens with pre-existing setup code

3. **Upload data files**:
   - Click **Data** in left sidebar
   - Click **Add** â†’ **Create Table**
   - Drag and drop `nyc-tripdata.csv`
   - Note the path (usually `/FileStore/tables/nyc-tripdata.csv`)
   - Repeat for `taxi_zone_lookup.csv`

4. **Copy code to Databricks**:
   - Open `q2_solution.py` in a text editor
   - Find each section marked `# CELL X:`
   - Copy that section into a new cell in your Databricks notebook
   - **Key changes needed**:
     - Remove `spark = SparkSession.builder...` (Databricks provides spark)
     - Change `show()` to `display()` (Databricks function)
     - Update file paths if needed

5. **Run all cells in order**:
   - Click **Run All** or run cell by cell
   - Verify outputs appear correctly
   - Take screenshots if needed

6. **Export results**:
   - For each task output, manually copy values to `q2_results.csv`
   - Open `q2_results.csv` in Excel or text editor
   - Paste values under correct sections
   - **DO NOT** modify structure or headers

7. **Export notebook files**:
   - Click **File** â†’ **Export** â†’ **DBC Archive** â†’ Save as `q2.dbc`
   - Click **File** â†’ **Export** â†’ **IPython Notebook** â†’ Save as `q2.ipynb`

8. **Submit to Gradescope**:
   - Upload `q2.dbc`
   - Upload `q2.ipynb`
   - Upload `q2_results.csv`

---

## ðŸ—ï¸ **Solutions Architect Perspective: Local â†’ Production**

### **1. Development Workflow Evolution**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Local Developmentâ”‚     â”‚ Databricks Dev   â”‚     â”‚ Production Cluster  â”‚
â”‚  (Your laptop)   â”œâ”€â”€â”€â”€â–ºâ”‚ Community Editionâ”œâ”€â”€â”€â”€â–ºâ”‚ (Company AWS/Azure) â”‚
â”‚  - Test logic    â”‚     â”‚ - Collaborate    â”‚     â”‚ - Scale to PB       â”‚
â”‚  - Debug fast    â”‚     â”‚ - Validate       â”‚     â”‚ - Auto-scaling      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Your Current Setup (HW3):**
- **Local Docker** with PySpark â†’ Test Q1
- **Databricks Community** â†’ Run Q2
- **Limited resources** (free tier)

#### **Company Production Setup:**
- **Development workspace** â†’ Data scientists write code
- **Staging workspace** â†’ QA team tests
- **Production workspace** â†’ Automated jobs run 24/7
- **Massive scale** â†’ Process terabytes per hour

---

### **2. Why Companies Use Databricks (Not Just Spark)**

| Feature | Local Spark | Open-Source Spark Cluster | Databricks Platform |
|---------|-------------|--------------------------|---------------------|
| **Setup Time** | 30 min | Days/weeks | 5 minutes |
| **Cluster Management** | Manual | Complex (Kubernetes/YARN) | Fully managed |
| **Auto-scaling** | âŒ No | Manual configuration | âœ… Automatic |
| **Notebook Collaboration** | âŒ No | Limited | âœ… Built-in (like Google Docs) |
| **Version Control** | Git only | Git only | âœ… Git + notebook versioning |
| **Job Scheduling** | Cron/Airflow | Airflow/Oozie | âœ… Built-in scheduler |
| **Monitoring & Logging** | Manual | Need to build | âœ… Integrated dashboards |
| **Cost Optimization** | Fixed | Manual | âœ… Auto-shutdown idle clusters |
| **Security & Governance** | âŒ DIY | âŒ DIY | âœ… Enterprise-grade |
| **ML Integration** | Basic | MLflow separately | âœ… MLflow built-in |
| **Delta Lake** | Add-on | Add-on | âœ… Native support |

---

### **3. Real-World Databricks Use Cases**

#### **Case Study 1: E-Commerce Company (Fortune 500)**

**Problem:**
- Processing 50TB of clickstream data daily
- Data scientists waiting 2 hours for Spark jobs
- DevOps team spending 40% time managing clusters

**Databricks Solution:**
```
Traditional Setup:
â”œâ”€â”€ Provision EC2 instances (30 min)
â”œâ”€â”€ Configure Spark cluster (1 hour)
â”œâ”€â”€ Deploy code (20 min)
â”œâ”€â”€ Monitor logs across nodes (ongoing pain)
â””â”€â”€ Scale up manually during peak (missed deadlines)

Databricks Setup:
â”œâ”€â”€ Click "Create Cluster" (2 min)
â”œâ”€â”€ Auto-scales 10-100 nodes based on load
â”œâ”€â”€ Runs job, auto-terminates
â””â”€â”€ Team focuses on analysis, not infrastructure
```

**Results:**
- **80% reduction** in infrastructure management time
- **60% cost savings** from auto-scaling and spot instances
- **10x faster** iteration for data scientists

---

#### **Case Study 2: Healthcare Analytics Startup**

**Challenge:**
- Small team (5 people)
- Need to process patient data (HIPAA compliant)
- Limited DevOps expertise

**Why Databricks:**
1. **Security built-in**:
   - SOC 2 Type II certified
   - HIPAA compliant
   - Encryption at rest and in transit
   - No need to build security from scratch

2. **Collaboration**:
   - Data engineers build ETL pipelines
   - Data scientists train ML models
   - All in same notebooks, version controlled

3. **Cost-effective**:
   - Pay only when clusters run
   - Community edition for testing
   - Scale up for production

---

### **4. Architecture: How Databricks Fits in Production**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Databricks Workspace                      â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Notebooks â”‚  â”‚  Jobs        â”‚  â”‚  ML Pipelines â”‚       â”‚
â”‚  â”‚  (Dev)     â”‚  â”‚  (Scheduled) â”‚  â”‚  (Training)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚         Spark Clusters (Auto-scaling)          â”‚         â”‚
â”‚  â”‚  â€¢ Dev cluster (small, always-on)              â”‚         â”‚
â”‚  â”‚  â€¢ Job clusters (spin up, run, terminate)      â”‚         â”‚
â”‚  â”‚  â€¢ ML clusters (GPU-enabled)                   â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                      â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Data Lake (S3/ADLS)   â”‚  â”‚  Delta Lake     â”‚
    â”‚   â€¢ Raw data            â”‚  â”‚  â€¢ Curated data â”‚
    â”‚   â€¢ Archives            â”‚  â”‚  â€¢ ACID trans   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Data Sources                      â”‚
    â”‚   â€¢ Databases (Postgres, MySQL)     â”‚
    â”‚   â€¢ APIs (REST, GraphQL)            â”‚
    â”‚   â€¢ Streaming (Kafka, Kinesis)      â”‚
    â”‚   â€¢ Files (CSV, Parquet, JSON)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **5. Cost Comparison: DIY vs Databricks**

#### **Scenario: Processing 1TB data/day**

**Option A: DIY Spark on AWS EC2**
```
Monthly Costs:
â”œâ”€â”€ EC2 instances (5 x m5.2xlarge, 24/7)  â†’ $5,700
â”œâ”€â”€ DevOps engineer (maintain cluster)     â†’ $15,000
â”œâ”€â”€ Data engineering time (fixing infra)   â†’ $5,000
â”œâ”€â”€ Storage (S3)                           â†’ $200
â””â”€â”€ TOTAL                                  â†’ $25,900/month
```

**Option B: Databricks on AWS**
```
Monthly Costs:
â”œâ”€â”€ Databricks Platform (compute + DBU)    â†’ $3,500
â”œâ”€â”€ Storage (S3)                           â†’ $200
â”œâ”€â”€ DevOps time (minimal)                  â†’ $1,000
â”œâ”€â”€ Data engineering time (focus on value) â†’ $0 (saved)
â””â”€â”€ TOTAL                                  â†’ $4,700/month

SAVINGS: $21,200/month (82% reduction)
```

**Why cheaper?**
1. **Auto-termination**: Clusters shut down when idle
2. **Spot instances**: Databricks automatically uses cheaper spot instances
3. **Efficiency**: No wasted time on infrastructure

---

### **6. From Your HW to Production: Evolution Path**

#### **Your HW3 Q2:**
```python
# Single notebook
# Static filter (trip_distance > 2.0)
# Manual copy to CSV
# One-time analysis
```

#### **Production Version:**
```python
# Scheduled job (runs daily at 2 AM)
# Dynamic filters from config table
# Automated export to data warehouse
# Alerts on anomalies
# Integration with BI tools (Tableau/Looker)
```

**What Changes:**

1. **Parameterization**:
```python
# HW Version (hardcoded):
tripdata_filtered = tripdata.filter(col("trip_distance") > 2.0)

# Production Version (configurable):
from dbutils.widgets import text
text("min_distance", "2.0")
min_dist = float(dbutils.widgets.get("min_distance"))
tripdata_filtered = tripdata.filter(col("trip_distance") > min_dist)
```

2. **Error Handling**:
```python
# HW Version:
task1a = tripdata.groupBy("DOLocationID").count()

# Production Version:
try:
    task1a = tripdata.groupBy("DOLocationID").count()
    assert task1a.count() > 0, "No data returned"
    task1a.write.mode("overwrite").saveAsTable("analytics.top_dropoffs")
except Exception as e:
    send_alert(f"Job failed: {e}")
    raise
```

3. **Data Quality Checks**:
```python
# Production adds:
- Null checks
- Schema validation
- Row count monitoring
- Duplicate detection
- Referential integrity
```

4. **Output Format**:
```python
# HW: Manual CSV copy
display(task1a)

# Production: Automated writes
task1a.write \
    .format("delta") \
    .mode("overwrite") \
    .option("mergeSchema", "true") \
    .saveAsTable("analytics.daily_dropoff_stats")
    
# Trigger downstream:
# - Update dashboard in Tableau
# - Send email report to stakeholders
# - Train ML model on new data
```

---

### **7. Enterprise Databricks Features (Beyond Your HW)**

#### **Unity Catalog** (Data Governance)
```
Enterprise Problem: "Who accessed what data? Is it secure?"

Unity Catalog Solution:
â”œâ”€â”€ Centralized metadata
â”œâ”€â”€ Fine-grained access control
â”œâ”€â”€ Audit logging (compliance)
â”œâ”€â”€ Data lineage tracking
â””â”€â”€ Cross-workspace governance
```

#### **Delta Live Tables** (ETL Automation)
```python
# Your HW: Imperative code
df = spark.read.csv("data.csv")
df_clean = df.filter(...)
df_clean.write.saveAsTable("clean_data")

# Production DLT: Declarative
@dlt.table
@dlt.expect_or_drop("valid_trip", "trip_distance > 0")
def clean_trips():
    return spark.read.csv("data.csv")
    
# DLT handles:
- Automatic refresh
- Data quality enforcement
- Dependency management
- Incremental processing
```

#### **MLflow Integration**
```python
# Track experiments
import mlflow
with mlflow.start_run():
    model = train_model(data)
    mlflow.log_metric("accuracy", 0.95)
    mlflow.sklearn.log_model(model, "model")
    
# Deploy to production with one click
```

---

### **8. Key Takeaways: Why Databricks Wins**

| Challenge | Traditional Approach | Databricks Approach |
|-----------|---------------------|---------------------|
| **Setup** | Days of configuration | 5 minutes |
| **Scaling** | Manual, error-prone | Automatic |
| **Collaboration** | Git conflicts | Real-time co-editing |
| **Cost** | Always-on clusters | Pay-per-use |
| **Security** | Build yourself | Enterprise-ready |
| **Monitoring** | Stitch tools together | Unified platform |
| **Maintenance** | Ongoing burden | Managed for you |

---

### **9. Career Perspective: Skills Transfer**

**What you're learning in HW3:**
- âœ… Spark DataFrame API (industry standard)
- âœ… ETL patterns (filter, join, aggregate)
- âœ… Window functions (advanced analytics)
- âœ… Performance thinking (minimize shuffles)

**Direct translation to job market:**
- **Role:** Data Engineer at tech company
- **Tech stack:** Databricks + Delta Lake + Airflow
- **Your HW experience:** "Built scalable data pipelines using Spark DataFrame API in Databricks"
- **Salary impact:** +$20-40K vs just knowing SQL

---

### **10. Next Steps After HW3**

1. **Get Databricks certification** (free with Community Edition):
   - Databricks Certified Associate Developer for Apache Spark
   - Resume booster

2. **Build portfolio project**:
   - Take HW3 code
   - Add real-time streaming component (Kafka)
   - Deploy on Databricks Community
   - Share GitHub link

3. **Learn production patterns**:
   - Delta Lake (ACID transactions)
   - Medallion architecture (bronze/silver/gold)
   - CI/CD for notebooks
   - Infrastructure as Code (Terraform + Databricks)

---

## ðŸ“Š **Summary: Your Learning Journey**

```
Week 1 (HW3)                  Year 1 (Job)                   Year 3 (Senior)
â””â”€â”€ Local Spark       â†’       Databricks Platform     â†’      Architecture Design
    â€¢ Basic queries           â€¢ Production pipelines          â€¢ System design
    â€¢ Small datasets          â€¢ TB-scale data                 â€¢ Cost optimization
    â€¢ Learning                â€¢ Team collaboration            â€¢ Tech leadership
```

**You're learning the foundation that 80% of big tech companies use for big data processing.** Databricks is the industry standard for a reason - it takes Spark (powerful but complex) and makes it accessible, collaborative, and cost-effective at scale.

---

## ðŸš€ **Ready to Implement?**

1. âœ… Created: `q2_solution.py` with all 6 tasks
2. âœ… Each section is clearly marked for copy-paste to Databricks
3. âœ… Uses ONLY DataFrame API (no SQL)
4. âœ… Ready for you to test locally or transfer directly to Databricks

**Good luck! You're building real-world production skills! ðŸŽ¯**

